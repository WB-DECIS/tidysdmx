{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Check configuration\n",
    "Should return path to correct python version (from virtual environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "# print('\\n'.join(sys.path[:6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules before execution of each cell\n",
    "# so when you edit src/mypackage/*.py in your editor and rerun cells, \n",
    "# changes appear immediately.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# python\n",
    "from __future__ import annotations\n",
    "\n",
    "# Standard library\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party\n",
    "from pysdmx.model import FixedValueMap, ImplicitComponentMap, ValueMap, MultiValueMap, ComponentMap\n",
    "from openpyxl import Workbook, load_workbook\n",
    "import pandas as pd\n",
    "import pysdmx as px\n",
    "import pickle as pkl\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Custom\n",
    "## Functions\n",
    "from tidysdmx import (\n",
    "    filter_tidy_raw, \n",
    "    validate_dataset_local, \n",
    "    map_structures, \n",
    "    apply_fixed_value_maps, \n",
    "    apply_implicit_component_maps, \n",
    "    build_date_pattern_map,\n",
    "    build_value_map_list,\n",
    "    build_multi_value_map_list,\n",
    "    build_representation_map,\n",
    "    build_single_component_map,\n",
    "    extract_component_ids,\n",
    "    write_excel_mapping_template,\n",
    "    build_structure_map,\n",
    "    create_schema_from_table,\n",
    "    build_structure_map_from_template_wb,\n",
    "    standardize_output,\n",
    "    parse_mapping_template_wb\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Define globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION! FOR TESTING ONLY. DO NOT USE IN PRODUCTION.\n",
    "# os.environ[\"PYTHONHTTPSVERIFY\"] = \"0\"\n",
    "\n",
    "# FMR and artefacts information\n",
    "fmr_url = \"https://fmrqa.worldbank.org/FMR/sdmx/v2\"\n",
    "# raw schema\n",
    "# raw_structure_agency = \"WB\"\n",
    "# raw_structure_id = \"IFPRI_ASTI\"\n",
    "# raw_structure_version = \"1.0\"\n",
    "# dissemination schema\n",
    "dis_structure_agency = \"WB.GGH.HSP\"\n",
    "dis_structure_id = \"DS_ASPIRE\"\n",
    "dis_structure_version = \"1.0.0\"\n",
    "# structure map\n",
    "# raw_structure_map = \"SM_IFPRI_ASTI_TO_DATA360\"\n",
    "\n",
    "# Path to raw data\n",
    "path_to_raw_data = Path(\n",
    "    \"../TMP/data/WB_ASPIRE\"\n",
    ")\n",
    "path_to_xlsx_mapping = Path(\n",
    "    \"./data/WB_ASPIRE_MODIFIED_COPY.xlsx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Initiate API client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fmr_url)\n",
    "client = px.api.fmr.RegistryClient(fmr_url)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# STEP 1 - Load raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Here we are loading the raw dataset as provided from the source. In this demonstration notebook, the raw data is simply being loaded from file, but in the final pipeline, the provenance of the file should be fully documented in a configuration file, and read from the source / DDH possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_data(folder_path):\n",
    "    \"\"\"Read all .csv files from a folder and return a single pandas DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str | pathlib.Path\n",
    "        Path to the folder containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Concatenated DataFrame of all CSVs (same structure assumed), with a\n",
    "        \"Series code\" column indicating the source filename (without .csv).\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    csv_files = sorted(folder.glob(\"*.csv\"))\n",
    "    if not csv_files:\n",
    "        raise ValueError(f\"No CSV files found in folder: {folder}\")\n",
    "\n",
    "    dfs = []\n",
    "    for f in csv_files:\n",
    "        df = pd.read_csv(f)\n",
    "        df[\"Series code\"] = f.stem  # filename without extension\n",
    "        dfs.append(df)\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "raw_df = read_raw_data(path_to_raw_data)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# STEP 2: Reshape raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "A critical step of this opinionated pipeline framework is to systematically reshape raw into tidy format (one observation per row). For more information about tidy data, please refer to [Hadley Wickam's original paper](https://vita.had.co.nz/papers/tidy-data.pdf). \n",
    "\n",
    "This step is critical because once data has been reshaped into a tidy format, the rest of the pipeline can be fully standradized, bringing immediate maintenance, scalability, and insititutional knowledge benefits. \n",
    "\n",
    "This is also a good place to implement minimal data cleaning if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_raw_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Reshape raw data and implements basic data cleaning.\n",
    "\n",
    "    It 'melts' (unpivots) columns starting with 'data.' into two columns ('name' and 'value'),\n",
    "    and then cleans the 'name' column by removing the 'data.' prefix.\n",
    "\n",
    "    Args:\n",
    "        df: The input pandas DataFrame containing columns like 'data.1', 'data.2', etc.\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame in the longer format.\n",
    "    \"\"\"\n",
    "    # 1. Equivalent of R's pivot_longer (using melt)\n",
    "    # Selects columns starting with 'data.' for unpivoting\n",
    "    data_cols = df.filter(like='YR').columns.tolist()\n",
    "\n",
    "    df_lg = df.melt(\n",
    "        id_vars=[col for col in df.columns if col not in data_cols], # Keep all non-data columns as identifier variables\n",
    "        var_name='year',    # New column for the original column names\n",
    "        value_name='value', # New column for the values\n",
    "    )\n",
    "\n",
    "    # 2. Equivalent of R's stringr::str_replace\n",
    "    # Removes the 'data.' prefix from the 'name' column\n",
    "    df_lg['year'] = df_lg['year'].str.replace('YR', '', regex=False)\n",
    "\n",
    "    # Filter out rows with missing values in 'value'\n",
    "    df_lg = df_lg.dropna(subset=['value'])\n",
    "    \n",
    "    return df_lg\n",
    "\n",
    "tidy_raw_df = reshape_raw_data(raw_df)\n",
    "tidy_raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# STEP 3: Describe the tidy raw data input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "We will describe the tidy raw data input using an SDMX schema. This description will allow for early validation of our input data during subsequent run of the pipelines for data updates. \n",
    "\n",
    "The `create_schema_from_table()` helper function allows pipeline developers to create pysdmx schema object automatically with minimal inputs from the pipeline developers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_raw_schema=create_schema_from_table(\n",
    "    tidy_raw_df, \n",
    "    dimensions=[\"Series code\", \"economy\"],\n",
    "    time_dimension=\"year\", \n",
    "    measure=\"value\")\n",
    "\n",
    "tidy_raw_schema.components[\"Series code\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# STEP 4: Filter out unnecessary rows (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_constraints(df: pd.DataFrame, constraints: Dict[str, List]) -> pd.DataFrame:\n",
    "#     \"\"\"Filters a DataFrame based on a dictionary of column names and valid values.\n",
    "    \n",
    "#     Args:\n",
    "#         df (pd.DataFrame): The source dataframe.\n",
    "#         constraints (dict): A dict where keys are column names and values are \n",
    "#                         lists of valid entries to keep (e.g., {'col': ['val1', 'val2']}).\n",
    "                        \n",
    "#     Returns:\n",
    "#         pd.DataFrame: A filtered copy of the original dataframe.\n",
    "#     \"\"\"\n",
    "#     for column, valid_values in constraints.items():\n",
    "#         # strict check: ensure column exists to avoid KeyErrors\n",
    "#         if column in df.columns:\n",
    "#             df = df[df[column].isin(valid_values)]\n",
    "#         else:\n",
    "#             print(f\"Warning: Column '{column}' not in DataFrame. Skipping.\")\n",
    "            \n",
    "#     return df\n",
    "\n",
    "\n",
    "# constraints = {\n",
    "#     \"Series code\": [\"per_allsp.adq_ep_preT_tot\", \"per_allsp.adq_ep_tot\", \"per_allsp.adq_pop_preT_tot\"]#,\n",
    "#     # \"TIME_PERIOD\": [\"1992\"],\n",
    "#     # \"AREA\": [\"GHA\"]\n",
    "# }\n",
    "\n",
    "\n",
    "# tidy_raw_df=apply_constraints(tidy_raw_df, constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# STEP 5: Create structure map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Fetch dissemination schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_schema = client.get_schema(\"datastructure\", agency=dis_structure_agency, id=dis_structure_id, version=dis_structure_version)\n",
    "dis_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Create structure map from mapping template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = parse_mapping_template_wb(path_to_xlsx_mapping)\n",
    "sm=build_structure_map_from_template_wb(mappings)\n",
    "sm.maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# Map data to dissemination schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Implement mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped = map_structures(df = tidy_raw_df, structure_map = sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Standardize output for upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "artefact_id=dis_structure_agency + \":\" + dis_structure_id + \"(\" + dis_structure_version + \")\"\n",
    "\n",
    "out = standardize_output(\n",
    "    df=mapped,\n",
    "    artefact_id=artefact_id,\n",
    "    schema=dis_schema,\n",
    "    action=\"I\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "# STEP 6: Final validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_errors = validate_dataset_local(df = out, schema = dis_schema)#, sdmx_cols=[])\n",
    "dis_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "# Testing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = mapped[mapped['INDICATOR'].isna()][['Series code', 'INDICATOR']].drop_duplicates(subset=['Series code'])\n",
    "errors.to_csv(\"wb_aspire_mapping_errors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_indicators = tidy_raw_df[['Series code']].drop_duplicates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tidysdmx (dev)",
   "language": "python",
   "name": "tidysdmx"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
